from llama_cpp import Llama
import os
import json

# –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Llama
MODEL_PATH = os.path.join("models", "Llama-3.2-3B-Instruct-Q4_K_M.gguf")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
_llm = None

def get_llm():
    global _llm
    if _llm is None:
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {MODEL_PATH}")
        
        print(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ LLM {MODEL_PATH}...")
        _llm = Llama(
            model_path=MODEL_PATH,
            n_ctx=2048,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
            n_threads=4, # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (–Ω–∞—Å—Ç—Ä–æ–π –ø–æ–¥ —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä)
            verbose=False
        )
    return _llm

def get_city_explanation(score, ndvi, infrastructure, user_type):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–Ω—è—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–π–æ–Ω–∞.
    """
    llm = get_llm()
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    infra_list = [f"{item['name']} ({item['type']})" for item in infrastructure[:10]]
    infra_str = ", ".join(infra_list)
    
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —É—Ä–±–∞–Ω–∏—Å—Ç–∏–∫–µ –∏–∑ –∫–æ–º–∞–Ω–¥—ã CityVibe AI. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±—ä—è—Å–Ω–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ—á–µ–º—É –µ–≥–æ —Ä–∞–π–æ–Ω –ø–æ–ª—É—á–∏–ª –æ—Ü–µ–Ω–∫—É {score}/100.
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ:
- –ò–Ω–¥–µ–∫—Å –æ–∑–µ–ª–µ–Ω–µ–Ω–∏—è (NDVI): {ndvi}%
- –¢–∏–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_type}
- –û–±—ä–µ–∫—Ç—ã —Ä—è–¥–æ–º: {infra_str}
<|eot_id|><|start_header_id|>user<|end_header_id|>
–û–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É –º–æ–π —Ä–∞–π–æ–Ω –ø–æ–ª—É—á–∏–ª {score} –±–∞–ª–ª–æ–≤? –ö–∞–∫–∏–µ –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã?<|eot_id|><|start_header_id|>assistant<|end_header_id|>"""

    response = llm(
        prompt,
        max_tokens=256,
        stop=["<|eot_id|>", "Task:"],
        echo=False
    )
    
    return response['choices'][0]['text'].strip()

if __name__ == "__main__":
    # –¢–µ—Å—Ç
    test_infra = [{"name": "–ú–∞–≥–Ω–∏—Ç", "type": "–ú–∞–≥–∞–∑–∏–Ω"}, {"name": "–ü–∞—Ä–∫ –ü–æ–±–µ–¥—ã", "type": "–ü–∞—Ä–∫"}]
    explanation = get_city_explanation(85, 45, test_infra, "resident")
    print(f"ü§ñ AI: {explanation}")
